{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15d97b16-a983-423c-8281-d626c750b23c",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/casadoj/efas_hydro.git/HEAD?urlpath=%2Fdoc%2Ftree%2F.%2Fnotebook%2Fglofas_calibration.ipynb)\n",
    "\n",
    "# GloFAS calibration time series\n",
    "\n",
    "This notebook creates a CSV file with the time series of daily discharge needed for the GloFAS calibration. First, it will extract the gauging stations available in the Hydrologial Data Management Service, and then it will download the discharge time series for those stations. As a result, it produces two ZIP files (_stations.zip_, _timeseries.zip_) that contain, respectively, a shapefile of the station metadata and the CSV file with the discharge time series for the selected stations and period.\n",
    "\n",
    "You can run the notebook in two ways. Locally, if you have cloned the repository and installed `EFAS_hydro` in your own environment, or virtually using Binder, so you don't need to download nor install anything. To proceed in this second way, click the ![Binder](https://mybinder.org/badge_logo.svg) logo on top; it will launch a notebook environment with all the necessary libraries; the files exported will be saved in the file browser of that virtual environment, so you will need to download them to your local machine.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5309e016-0cce-4af4-8b05-900a228b4516",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "from efashydro.stations import get_stations, plot_stations\n",
    "from efashydro.timeseries import get_timeseries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17f8c53-00ce-4dfd-83e7-a6fb2686b211",
   "metadata": {},
   "source": [
    "## Configuration \n",
    "\n",
    "In principle, the cell below is the only cell you need to edit in the notebook:\n",
    "\n",
    "1. Define your **credentials** (`USER` and `PASSWORD`) to be able to access the database.\n",
    "2. Define the filters used to select **stations**.\n",
    "   * The `KIND` of station will always be `'river'` in the GloFAS calibration.\n",
    "   * You can select stations by `COUNTRY_ID` (for instance `'IT'` for Italy) or `PROVIDER_ID`.\n",
    "   * Another option is to use a CSV file with a list of the stations of interest. The code will only read the first column in that file, which must contain the station ID in the database; the file must not have a header row. If you're running the notebook in Binder, drag and drop your CSV file into the file browser, so the code has access to it.\n",
    "4. Define the filters use to extract **time series** information. In principle, you don't need to edit this section, as GloFAS calibrations uses the 24 hour operational weighted (`'nhoperational24hw'`) time series of discharge (`'D'`) from 1980 to 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e059f4-2607-4c35-a1f7-090a0f586e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HDMS API configuration\n",
    "USER = 'xxxxxx'\n",
    "PASSWORD = 'yyyyyy'\n",
    "\n",
    "# station filters\n",
    "KIND = 'river'\n",
    "COUNTRY_ID = None\n",
    "PROVIDER_ID = None\n",
    "BASIN_NAME = None\n",
    "STATION_ID = 'stations.csv'\n",
    "\n",
    "# time series filters\n",
    "SERVICE = 'nhoperational24hw'\n",
    "VARIABLE = ['D']\n",
    "START = datetime(1980, 1, 1)\n",
    "END = datetime(2024, 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da5792a-16b1-42ca-a0a2-bb1d5a13a088",
   "metadata": {},
   "source": [
    "## `get_stations()`\n",
    "\n",
    "The following cell extracts the metadata of the stations in the database that pass the filters defined in the [Configuration](#Configuration) section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7498f18-b01e-4d6c-ac72-1eddb43a572e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(STATION_ID, str):\n",
    "    station_id = pd.read_csv(STATION_ID, usecols=[0], header=None).squeeze().tolist()\n",
    "\n",
    "stations = get_stations(\n",
    "    user=USER, \n",
    "    password=PASSWORD, \n",
    "    kind=KIND,\n",
    "    country_id=COUNTRY_ID,\n",
    "    provider_id=PROVIDER_ID,\n",
    "    basin_name=BASIN_NAME,\n",
    "    station_id=station_id,\n",
    ")\n",
    "print(f'Metadata for {len(stations)} stations were extracted')\n",
    "\n",
    "plot_stations(\n",
    "    geometry=stations.geometry,\n",
    "    area=stations.CATCH_SKM,\n",
    "    extent=[-180, 180, -60, 90] # edit to your study area [xmin, xmax, ymin, ymax]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb51043-ca90-4855-970a-dc7c062be203",
   "metadata": {},
   "source": [
    "The result is a `geopandas.GeoDataFrame` of stations and their metadata. As a `geopandas` object, the stations have associated their geographical location and can be exported to a shapefile to be used in a GIS software."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c51a57-51a5-4280-8f28-5dfa7f1c2550",
   "metadata": {},
   "source": [
    "### Export\n",
    "\n",
    "The cell below exports the station metadata in two formats (CSV and SHP) in a folder named _./stations/_. This folder is compressed into a **stations.zip** file, so you can easily download the results in case you're running the script in Binder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45eee927-4b41-4bae-86fc-d0fef829ca27",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_stations = Path('./stations/')\n",
    "path_stations.mkdir(parents=True, exist_ok=True)\n",
    "filename = 'stations'\n",
    "if COUNTRY_ID is not None:\n",
    "    filename += f'_{COUNTRY_ID}'\n",
    "if PROVIDER_ID is not None:\n",
    "    filename += f'_{PROVIDER_ID}'\n",
    "\n",
    "# as shapefile\n",
    "stations.to_file(path_stations / f'{filename}.shp')\n",
    "# as CSV\n",
    "stations.drop('geometry', axis=1).to_csv(path_stations / f'{filename}.csv')\n",
    "\n",
    "# compress the stations folder\n",
    "zipfile = shutil.make_archive('stations', 'zip', path_stations)\n",
    "print(f'You can now download the compressed file {zipfile} from the file browser.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348c74ff-19cf-42a3-99b2-f9e3db9c0469",
   "metadata": {},
   "source": [
    "## `get_timeseries()`\n",
    "\n",
    "The cell below extracts the daily time series of discharge for the stations selected above. Go to the [Configuration](#Configuration) section if you need to change the settings of the time series extraction: `SERVICE`, `VARIABLE`, `START`, `END`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e260f2-bbfa-4e76-ac1a-70cb05306de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series = {}\n",
    "for efas_id in tqdm(stations.index, desc='Stations'):\n",
    "    time_series[efas_id] = get_timeseries(\n",
    "        user=USER,\n",
    "        password=PASSWORD,\n",
    "        station_id=efas_id,\n",
    "        service=SERVICE,\n",
    "        variable=VARIABLE, \n",
    "        start=START,\n",
    "        end=END\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b9692e-f06c-470b-b7e4-7b619cb9db5c",
   "metadata": {},
   "source": [
    "The result is a dictionary of `pandas.DataFrames`, where every key is the ID of a station and the value the daily discharge time series available for that station. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed41f85-abb9-40e2-8f5e-3bc89ae7fb30",
   "metadata": {},
   "source": [
    "### Export\n",
    "\n",
    "The individual station time series are concatenated into a single `pandas.DataFrame` that contains the daily discharge time series for all the selected stations in the period from `START` to `END`. If there is no data at the beginning or end of the study period for none of the stations, the CSV file will still contain the whole period.\n",
    "\n",
    "The concatenated `pandas.DataFrame` is saved as CSV files inside the _./timeseries/_ folder. This folder is compressed into the **timeseries.zip** file to ease the download in case you're running the notebook in Binder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ebd757-f4ed-4aa2-8770-c12e42de7fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_timeseries = Path('./timeseries/')\n",
    "path_timeseries.mkdir(parents=True, exist_ok=True)\n",
    "filename = 'discharge'\n",
    "if COUNTRY_ID is not None:\n",
    "    filename += f'_{COUNTRY_ID}'\n",
    "if PROVIDER_ID is not None:\n",
    "    filename += f'_{PROVIDER_ID}'\n",
    "\n",
    "# concatenate all the time series\n",
    "ts_list = []\n",
    "for efas_id, df in time_series.items():\n",
    "    df.columns = [efas_id]\n",
    "    ts_list.append(df)\n",
    "ts_df = pd.concat(ts_list, axis=1)\n",
    "\n",
    "# make sure the time series cover all the period\n",
    "dates = pd.date_range(START, END, freq='D')\n",
    "if len(ts_df) != len(dates):\n",
    "    ts_df = ts_df.reindex(dates)\n",
    "    ts_df.index.name = 'time'\n",
    "\n",
    "# save as CSV file\n",
    "ts_df.to_csv(path_timeseries / f'{filename}.csv')\n",
    "\n",
    "# compress the time series folder\n",
    "zipfile = shutil.make_archive('timeseries', 'zip', path_timeseries)\n",
    "print(f'You can now download the compressed file {zipfile} from the file browser.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
